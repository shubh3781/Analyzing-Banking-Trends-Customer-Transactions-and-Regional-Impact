{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyzing Banking Trends: Customer Transactions and Regional Impact\n",
    "Embark on a thrilling data-driven adventure with Emily, the intrepid data explorer, as she sets sail into the ever-evolving world of banking and finance in our project, \"Analyzing Banking Trends: Customer Transactions and Regional Impact.\" With Python as her trusted compass and SQL as her guiding star, Emily delves headfirst into the treasure troves of transaction data, unearthing valuable insights into customer behavior patterns and their implications on different world regions.\n",
    "\n",
    "Emily is armed with unwavering determination and a keen eye for detail. She first meticulously polishes the raw dataset, ensuring it gleams with brilliance, ready for the ultimate exploration. Once her data shines brightly, SQL queries become her diving gear, enabling her to plunge into the depths of interconnected tables housing the secrets of customer transactions and their regional impact.\n",
    "\n",
    "But this journey isn't just data analysis; it's an epic expedition of discovery. The insights she uncovers are the guiding light for banking professionals, decision-makers, and strategists as they navigate the complexities of customer behavior and its far-reaching consequences on regional economies and financial systems.\n",
    "\n",
    "By the end of this adventure, Emily isn't just crunching numbers; she has unlocked the transformative power of data-driven insights. It's not just data; it's the key that unlocks the door to the future of banking and finance.\n",
    "\n",
    "Join Emily on this captivating voyage, where every line of code and every SQL query unravels the mysteries of banking trends. Together, we'll illuminate the intricate web of customer transactions and regional impact, revealing insights that will help shape the destiny of the banking and finance industry. The world of banking trends and regional impacts awaitsâ€”let's embark on this data-driven adventure and discover its hidden treasures that will change the landscape of banking forever.\n",
    "\n",
    "Module 1\n",
    "Task 1: Data Import and Initial Exploration\n",
    "In this phase, our objective is to efficiently import and scrutinize the data from the 'user_nodes.csv' file. By doing so, we lay the foundation for our data analysis journey. This initial step is pivotal as it empowers us to access the raw information we'll be working with, enabling us to gain valuable insights and make informed decisions. A comprehensive examination of the dataset's structure, contents, and potential data quality issues is vital for ensuring the reliability of our subsequent analyses. Through meticulous data import and initial exploration, we pave the way for a successful and insightful project.\n",
    "\n",
    "#--- Import Pandas ---\n",
    "import pandas as pd\n",
    "\n",
    "#--- Read in dataset (user_nodes.csv) ---\n",
    "\n",
    "df = pd.read_csv('./user_nodes.csv')\n",
    "\n",
    "#--- Inspect data ---\n",
    "df.head()\n",
    "Task 2: Identifying Null Values\n",
    "In this task, our focus shifts to identifying and quantifying null values within our dataset. The count of null values (referred to as 'null_values' in the code) plays a pivotal role in our data preprocessing efforts. It provides us with essential insights into the completeness and quality of the data, ensuring that we are aware of any missing information that may impact our analyses. By acknowledging and handling null values appropriately, we pave the way for robust and accurate data-driven conclusions, free from the potential distortions caused by incomplete data.\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 2 ---\n",
    "null_values = df.isnull().sum()\n",
    "\n",
    "#--- Inspect data ---\n",
    "null_values\n",
    "Task 3: Identifying Duplicate Data\n",
    "In this task, we aim to identify and quantify the presence of duplicate data within our dataset. The count of duplicates (referred to as 'duplicates' in the code) is an important metric. It helps us understand the extent of redundancy in our dataset, which is crucial for data quality and accuracy in our analysis. By recognizing and handling duplicate records, we ensure that our insights and conclusions are based on unique, meaningful data, preventing any potential distortions caused by repeated entries.\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 3 ---\n",
    "duplicates = df.duplicated().sum()\n",
    "\n",
    "#--- Inspect data ---\n",
    "duplicates\n",
    "Task 4: Removing Duplicate Data\n",
    "Building upon our previous task's findings, we now take action to eliminate duplicate records within our dataset. By executing the code that removes duplicates, we enhance the quality and integrity of our data. This step is pivotal to ensure that our analyses and results are based on unique, non-redundant information, safeguarding our project from potential biases introduced by repeated entries. The removal of duplicates is a critical component in our data preparation process, promoting the generation of accurate and reliable insights.\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 4 ---\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "#--- Inspect data ---\n",
    "df.head()\n",
    "Task 5: Data Transformation and Cleanup\n",
    "In this task, we embark on a transformative journey with our dataset. Our primary goal is to enhance the data's usability and interpretability by removing specific columns and renaming others. By executing the provided code, we remove the 'has_loan' and 'is_act' columns, streamlining the data for more focused analysis. Furthermore, we redefine column names for clarity, and the dataset is saved to a new CSV file named 'user_nodes_cleaned.csv'. This meticulous data transformation and cleanup process prepares our data for more effective and insightful analyses, ensuring that it aligns with our project's objectives and requirements.\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 5 ---\n",
    "\n",
    "# Create a list of columns to remove\n",
    "#columns_to_remove = [\"has_loan\", \"is_act\"]\n",
    "\n",
    "# Remove the specified columns\n",
    "df.drop(columns=[\"has_loan\", \"is_act\"], axis=1, inplace=True)\n",
    "\n",
    "# Define a dictionary to rename columns\n",
    "\n",
    "\n",
    "# Rename the specified columns\n",
    "df.rename(columns= {\n",
    "    'id_': 'consumer_id',\n",
    "    'area_id_': 'region_id',\n",
    "    'node_id_': 'node_id',\n",
    "    'act_date': 'start_date',\n",
    "    'deact_date': 'end_date'\n",
    "}, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "#df.to_csv('user_nodes_cleaned.csv', index=False)\n",
    "\n",
    "#--- Inspect data ---\n",
    "df.head()\n",
    "Task 6: Importing Transaction Data\n",
    "In this task, we take the crucial step of importing transaction data from the 'user_transactions.csv' file. By loading this data into our project, we expand our dataset's breadth and open the doors to deeper analysis and insights. The imported data, represented by the DataFrame 'df1', provides additional context and information for our project, enabling us to conduct a more comprehensive and detailed analysis. This data import is a pivotal element in our overall project, laying the foundation for more informed and meaningful conclusions.\n",
    "\n",
    "#--- Import Pandas ---\n",
    "import pandas as pd\n",
    "\n",
    "#--- Read in dataset (user_transactions.csv) ----\n",
    "\n",
    "df1 = pd.read_csv('./user_transactions.csv')\n",
    "\n",
    "#--- Inspect data ---\n",
    "df1.head()\n",
    "Task 7: Identifying Null Values in Transaction Data\n",
    "In this task, we shift our focus to the recently imported transaction data, 'df1'. Our objective is to identify and quantify null values within this dataset. The 'null_values' count, as shown in the code, serves as a critical metric to assess the data's completeness and quality. Detecting and addressing null values is essential to ensure that our analyses are based on complete and reliable information. This thorough examination of null values in the transaction data is integral to maintaining data integrity and making well-informed decisions in our project\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 7 ---\n",
    "null_value = df1.isnull().sum()\n",
    "\n",
    "#--- Inspect data ---\n",
    "null_value\n",
    "Task 8: Identifying Duplicate Data in Transaction Data\n",
    "In this task, our attention is directed towards the recently imported transaction data, 'df1'. Our goal is to identify and quantify the presence of duplicate data within this dataset. The count of duplicates (referred to as 'duplicates' in the code) is a crucial metric that informs us about the extent of redundancy in our transaction records. By detecting and handling duplicate entries, we ensure the integrity of our analyses and conclusions, guaranteeing that they are founded on distinct and meaningful data. This process is vital for preventing any potential distortions introduced by replicated transactions in our project.\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 8 ---\n",
    "duplicate = df1.duplicated().sum()\n",
    "\n",
    "#--- Inspect data ---\n",
    "duplicate\n",
    "Task 9: Removing Duplicate Data in Transaction Data\n",
    "Building on the findings from our previous task, we now take action to remove duplicate transaction records within the 'df1' dataset. By executing the provided code, we enhance the quality and reliability of our transaction data. Eliminating duplicate entries is instrumental in ensuring that our analyses are based on unique and non-redundant information. This process is a vital step in our data preparation, as it promotes the generation of accurate and dependable insights in our project. The removal of duplicates safeguards our analyses from potential biases or inaccuracies caused by repeated transaction records.\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 9 ---\n",
    "\n",
    "df1.drop_duplicates(inplace = True)\n",
    "\n",
    "#--- Inspect data ---\n",
    "df1.head()\n",
    "Task 10: Transforming and Cleaning Transaction Data\n",
    "In this task, we embark on the transformation and cleaning of our transaction data, represented by the 'df1' DataFrame. Our primary objective is to enhance the data's relevance and clarity by removing specific columns and renaming others. By executing the provided code, we eliminate the 'has_credit_card' and 'account_type' columns, streamlining the data for a more focused analysis. Additionally, we redefine column names for better interpretability. The cleaned transaction data is then saved to a new CSV file named 'user_nodes_cleaned.csv'. This meticulous data transformation and cleanup process ensures that our transaction data aligns with our project's objectives and requirements, preparing it for more effective and insightful analyses.\n",
    "\n",
    "# --- WRITE YOUR CODE FOR TASK 10 ---\n",
    "# Remove the specified columns\n",
    "df1.drop(columns=[\"has_credit_card\", \"account_type\"], axis=1, inplace=True)\n",
    "\n",
    "# Rename the specified columns\n",
    "df1.rename(columns= {\n",
    "    'id_' : 'consumer_id',\n",
    "    't_date' : 'transaction_date',\n",
    "    't_type' : 'transaction_type',\n",
    "    't_amt' : 'transaction_amount'\n",
    "}, inplace=True)\n",
    "\n",
    "#--- Export the df1 as \"user_transactions_cleaned.csv\" ---\n",
    "df1.to_csv('user_transactions_cleaned.csv' , index=False)\n",
    "\n",
    "#--- Inspect data ---\n",
    "df1.head()\n",
    "Module 2\n",
    "Task 1: Data Download, Import, and Database Connection\n",
    "# -- Load the sql extention ----\n",
    "%load_ext sql\n",
    "\n",
    "# --- Load your mysql db using credentials from the \"DB\" area ---\n",
    "%sql mysql+pymysql://bcbc982a:Cab#22se@localhost/bcbc982a\n",
    "Task 2: Banking Regional Insights\n",
    "This task aims to analyze the distribution of bank users across regions, utilizing the 'world_regions' and 'user_nodes' datasets. By understanding the regional user landscape, we can tailor services, identify growth opportunities, and customize strategies. The insights obtained will empower banks to make data-driven decisions, ensuring they serve customers effectively and stay competitive in different regions.\n",
    "\n",
    "%%sql\n",
    "SELECT wr.region_name, count(Distinct un.consumer_id) as num_users\n",
    "from world_regions as wr\n",
    "LEFT JOIN user_nodes as un\n",
    "on wr.region_id = un.region_id\n",
    "group by region_name;\n",
    "Task 3: Identifying Largest Deposit User\n",
    "This task involves identifying the user who made the largest deposit in the 'user_transaction' dataset and determining the transaction type associated with that deposit. By selecting the transaction record with the maximum transaction amount, we can pinpoint the user responsible for the largest deposit and uncover the transaction type, providing valuable insights into significant financial activities.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    consumer_id, \n",
    "    transaction_type, \n",
    "    transaction_amount AS largest_deposit\n",
    "FROM \n",
    "    user_transaction\n",
    "WHERE \n",
    "    transaction_amount = (\n",
    "        SELECT \n",
    "            MAX(transaction_amount)\n",
    "        FROM \n",
    "            user_transaction\n",
    "        WHERE \n",
    "            transaction_type = 'deposit'\n",
    "    )\n",
    "    AND transaction_type = 'deposit';\n",
    "Task 4: Total Deposits in Europe by User\n",
    "In this task, we calculate the total deposit amount for each user within the \"Europe\" region. By joining the 'user_nodes,' 'user_transaction,' and 'world_regions' datasets, and filtering for 'deposit' transactions in the specified region, we derive the sum of deposit amounts for each user. This analysis provides an overview of deposit activities in Europe, offering insights into individual user deposit patterns.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    un.consumer_id, \n",
    "    SUM(ut.transaction_amount) AS total_deposit_amount\n",
    "FROM \n",
    "    user_transaction ut\n",
    "left JOIN \n",
    "    user_nodes as un ON ut.consumer_id = un.consumer_id\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "WHERE \n",
    "    wr.region_name = 'Europe' \n",
    "    AND ut.transaction_type = 'deposit'\n",
    "GROUP BY \n",
    "    ut.consumer_id;\n",
    "Task 5: Total Transactions by User in the United States\n",
    "In this task, we determine the total number of transactions made by each user within the \"United States\" region. By joining the 'user_nodes,' 'user_transaction,' and 'world_regions' datasets and filtering for transactions in the specified region, we calculate the count of transactions for each user. This analysis offers valuable insights into user transaction behavior and activity levels in the United States region.\n",
    "\n",
    "%%sql\n",
    "SELECT un.consumer_id,\n",
    "    count(un.consumer_id) num_transactions\n",
    "FROM \n",
    "    user_transaction ut\n",
    "left JOIN \n",
    "    user_nodes as un ON ut.consumer_id = un.consumer_id\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "WHERE \n",
    "    wr.region_name = 'United States' \n",
    "GROUP BY \n",
    "    ut.consumer_id;\n",
    "Task 6: Users with More Than 5 Transactions\n",
    "This task involves calculating the total number of users who have made more than 5 transactions. By querying the 'user_transaction' dataset and using the HAVING clause to filter for users with transaction counts exceeding 5, we can identify and count the users meeting this criterion. This analysis provides insights into the number of active users with a significant transaction history.\n",
    "\n",
    "%%sql\n",
    "select\n",
    "consumer_id,\n",
    "count(consumer_id) as num_transactions\n",
    "from user_transaction\n",
    "group by consumer_id\n",
    "having count(consumer_id) > 5\n",
    "order by consumer_id\n",
    "Task 7: Regions with the Highest Node Counts\n",
    "In this task, we aim to identify and list the regions with the highest number of nodes assigned to them. By utilizing the 'world_regions' and 'user_nodes' datasets and grouping the data by region, we can calculate the node count in each region. The results are then sorted in descending order by node count, revealing the regions with the most nodes assigned to them. This analysis sheds light on the regions that are most densely populated with nodes, providing insights into network distribution.\n",
    "\n",
    "%%sql\n",
    "Select\n",
    "wr.region_name,\n",
    "count(un.node_id) as num_nodes\n",
    "from user_nodes as un\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "    group by wr.region_name\n",
    "    order by num_nodes desc\n",
    "Task 8: Largest Deposit in the Australia Region\n",
    "In this task, our objective is to identify the user who made the largest deposit amount in the \"Australia\" region. By querying the 'user_transaction' dataset and joining it with 'user_nodes' and 'world_regions' to filter for the specified region, we select users with deposit transactions and find the maximum deposit amount. The results are then sorted in descending order by deposit amount, and the user with the largest deposit is determined. This analysis provides insights into the most significant deposit activity within the Australia region.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    un.consumer_id, \n",
    "    max(ut.transaction_amount) AS largest_depo\n",
    "FROM \n",
    "    user_transaction as ut\n",
    "left JOIN \n",
    "    user_nodes as un ON ut.consumer_id = un.consumer_id\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "WHERE \n",
    "    wr.region_name = 'Australia' \n",
    "    AND ut.transaction_type = 'deposit'\n",
    "GROUP BY \n",
    "    ut.consumer_id\n",
    "Order by\n",
    "    largest_depo desc\n",
    "limit 1;\n",
    "Task 9: Total Deposits by User and Region\n",
    "In this task, we calculate the total deposit amount made by each user in each region. By joining the 'user_nodes,' 'world_regions,' and 'user_transaction' datasets and filtering for 'deposit' transactions, we group the data by consumer and region, summing up the deposit amounts for each combination. This analysis provides valuable insights into the deposit activities of individual users across different regions, allowing for a comprehensive overview of regional deposit patterns.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    un.consumer_id,wr.region_name, \n",
    "    SUM(ut.transaction_amount) AS total_deposit_amount\n",
    "FROM \n",
    "    user_transaction as ut\n",
    "left JOIN \n",
    "    user_nodes as un ON ut.consumer_id = un.consumer_id\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "WHERE  \n",
    "    ut.transaction_type = 'deposit'\n",
    "GROUP BY \n",
    "    ut.consumer_id,wr.region_name;\n",
    "Task 10: Total Transactions by Region\n",
    "This task focuses on retrieving the total number of transactions for each region. By joining the 'world_regions,' 'user_nodes,' and 'user_transaction' datasets, and grouping the data by region, we count the transactions made within each region. This analysis offers insights into transaction activity levels in different regions, providing a comprehensive view of regional transaction volumes.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    wr.region_name,\n",
    "    count(ut.transaction_amount) AS total_deposit_amount\n",
    "FROM \n",
    "    user_transaction ut\n",
    "left JOIN \n",
    "    user_nodes as un ON ut.consumer_id = un.consumer_id\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "GROUP BY \n",
    "    wr.region_name;\n",
    "Task 11: Regional Deposit Analysis\n",
    "This task aims to calculate the total deposit amount for each region listed in the 'user_transaction' table while considering only transactions associated with valid regions from the 'user_nodes' table. By joining these datasets and filtering for 'deposit' transactions, we gain insights into the distribution of deposit activity across various regions. This analysis enables financial institutions to understand which regions have the highest deposit volumes, allowing for tailored strategies and resource allocation to meet specific regional banking needs efficiently.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    wr.region_name,\n",
    "    sum(ut.transaction_amount) AS total_deposit_amount\n",
    "FROM \n",
    "    user_transaction ut\n",
    "left JOIN \n",
    "    user_nodes as un ON ut.consumer_id = un.consumer_id\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "    WHERE  \n",
    "    ut.transaction_type = 'deposit'\n",
    "GROUP BY \n",
    "    wr.region_name;\n",
    "Task 12: Top 5 Consumers by Total Deposit Amount\n",
    "In this task, we aim to identify the top 5 consumers who have made the highest total transaction amounts, which include the sum of all their deposit transactions in the 'user_transaction' table. By selecting deposit transactions, grouping the data by consumer, and sorting the results in descending order of total transaction amount, we can pinpoint the consumers with the most significant deposit activity. This analysis provides insights into the highest-value customers, helping financial institutions tailor their services and engage with their most valuable clientele.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    ut.consumer_id, \n",
    "    sum(ut.transaction_amount) AS total_deposit_amount\n",
    "FROM \n",
    "    user_transaction as ut\n",
    "WHERE  \n",
    "    ut.transaction_type = 'deposit'\n",
    "GROUP BY \n",
    "    ut.consumer_id\n",
    "order by\n",
    "    total_deposit_amount desc\n",
    "limit 5 ;\n",
    "Task 13: Consumer Allocation by Region\n",
    "This task involves determining the number of consumers allocated to each region. By combining data from the 'user_nodes' and 'world_regions' datasets and utilizing COUNT(DISTINCT) to count unique consumer IDs, we can provide insights into the consumer distribution across various regions. The results, sorted in descending order of consumer count, offer valuable information about the regional customer base, assisting businesses and institutions in regional resource allocation and tailored services.\n",
    "\n",
    "%%sql\n",
    "Select\n",
    "wr.region_id,\n",
    "wr.region_name,\n",
    "count(distinct un.consumer_id) as num_consumers\n",
    "from user_nodes as un\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "    group by wr.region_id,wr.region_name\n",
    "    order by num_consumers desc\n",
    "Task 14: Transaction Type Analysis: Unique Counts and Total Amount\n",
    "This task focuses on providing unique counts and total transaction amounts for each transaction type in the 'user_transaction' dataset. By grouping the data by transaction type and using COUNT(DISTINCT) for unique counts and SUM for total amounts, this analysis delivers insights into the distribution of transactions across different types. The results assist in understanding transaction behavior and patterns, aiding in strategic decisions and financial reporting.\n",
    "\n",
    "%%sql\n",
    "select transaction_type ,\n",
    "count(Distinct consumer_id) as unique_counts,\n",
    "sum(transaction_amount) as total_amounts\n",
    "from user_transaction\n",
    "group by transaction_type;\n",
    "Task 15: Average Deposit Counts and Amounts by Transaction Type\n",
    "In this task, we compute the average deposit counts and amounts for each transaction type ('deposit') across all customers. Using a common table expression (CTE) to summarize deposit data, we then calculate the average deposit counts and amounts, rounding the values for clarity. This analysis provides insights into the typical deposit behavior for each transaction type and can assist financial institutions in understanding customer deposit patterns and optimizing their services.\n",
    "\n",
    "%%sql\n",
    "WITH deposit_summary AS (\n",
    "    SELECT \n",
    "        transaction_type,\n",
    "        COUNT(consumer_id) AS deposit_count,\n",
    "        SUM(transaction_amount) AS deposit_amount\n",
    "    FROM \n",
    "        user_transaction\n",
    "    WHERE \n",
    "        transaction_type = 'deposit'\n",
    "    GROUP BY \n",
    "        consumer_id, transaction_type\n",
    ")\n",
    "SELECT \n",
    "    transaction_type AS txn_type,\n",
    "    ROUND(AVG(deposit_count)) AS avg_deposit_count,\n",
    "    ROUND(AVG(deposit_amount)) AS avg_deposit_amount\n",
    "FROM \n",
    "    deposit_summary\n",
    "GROUP BY \n",
    "    transaction_type;\n",
    "Task 16: Transaction Counts by Region\n",
    "This task is focused on determining the number of transactions made by consumers in each region. By joining the 'user_transaction,' 'user_nodes,' and 'world_regions' datasets and grouping the data by region, we calculate the transaction count in each region. This analysis provides insights into transaction activity levels in different regions, aiding in resource allocation and regional strategy decisions for financial institutions.\n",
    "\n",
    "%%sql\n",
    "SELECT \n",
    "    wr.region_name,\n",
    "    count(ut.consumer_id) AS total_transaction_count\n",
    "FROM \n",
    "    user_transaction ut\n",
    "left JOIN \n",
    "    user_nodes as un ON ut.consumer_id = un.consumer_id\n",
    "left JOIN \n",
    "    world_regions as wr ON un.region_id = wr.region_id\n",
    "    \n",
    "GROUP BY \n",
    "    wr.region_name;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
